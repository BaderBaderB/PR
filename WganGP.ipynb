{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe31870-8c63-4541-a92d-c4ec44df6c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.layers import Dropout \n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.initializers import HeNormal,GlorotUniform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from traitement_image import image_processing\n",
    "from utils import load_parquet_files_cond_red_save, wasserstein_loss, generator_loss, generate_batches_cond, generate_cond, Conv2DCircularPadding, load_parquet_files_cond_red, newTorchSign, transform_batch\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74813ec-163f-4d4f-8ebb-d5050559f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseInjection(layers.Layer):\n",
    "    def __init__(self, stddev=0.1, **kwargs):\n",
    "        super(NoiseInjection, self).__init__(**kwargs)\n",
    "        self.stddev = stddev  # Écart-type du bruit gaussien\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Cette méthode est utilisée pour créer les poids de la couche, si nécessaire.\n",
    "        # Dans ce cas, nous n'avons pas de poids à créer.-\n",
    "        super(NoiseInjection, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            # Générer du bruit gaussien avec la même forme que les entrées\n",
    "            noise = tf.random.normal(shape=tf.shape(inputs), mean=0.0, stddev=self.stddev)\n",
    "            # Ajouter le bruit aux entrées\n",
    "            return inputs + noise\n",
    "        return inputs  # Ne pas ajouter de bruit pendant l'inférence\n",
    "\n",
    "    def get_config(self):\n",
    "        # Permet de sérialiser la couche pour la sauvegarde et le chargement\n",
    "        config = super(NoiseInjection, self).get_config()\n",
    "        config.update({'stddev': self.stddev})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cbf469-afde-49a9-bf9f-5fc1645ed23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(models.Model):\n",
    "    \"\"\"\n",
    "    Définition du générateur\n",
    "\n",
    "    inputs :\n",
    "        latent_dim : dimension de l'espace latent, à faire varier en fonction de la variance souhaitée des échantillones\n",
    "        cond_dim : dimension de la signature qui sera concaténée à l'espace latent pour obtenir le vecteur permettant la génération\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, cond_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.cond_dim = cond_dim\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = models.Sequential()\n",
    "        # Adapter la taille de la condition en entrée\n",
    "        # model.add(layers.Dense(self.latent_dim, input_dim=self.cond_dim, activation='relu'))\n",
    "        model.add(layers.Dense(4 * 4 * 256, input_dim=self.latent_dim + self.cond_dim, activation='relu'))\n",
    "        model.add(layers.Reshape((4, 4, 256)))\n",
    "        model.add(layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding='same', activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Conv2DTranspose(16, kernel_size=4, strides=2, padding='same', activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(layers.UpSampling2D())\n",
    "        model.add(layers.Conv2D(1, kernel_size=2, padding='same', activation='sigmoid'))\n",
    "        # model.add(layers.Conv2D(1, kernel_size=3, padding='same', activation='sigmoid'))\n",
    "\n",
    "        return model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        latent, cond = inputs\n",
    "        input_model = layers.Concatenate()([latent, cond])\n",
    "        return self.model(input_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4d679c-32f8-4a81-9300-2bb5655f65b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(models.Model):\n",
    "    def __init__(self, cond_dim):\n",
    "        \"\"\"\n",
    "        Modèle du discriminateur\n",
    "        :param cond_dim: dimension de la signature (condition) en entrée\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.cond_dim = cond_dim\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        image_input = layers.Input(shape = (128, 128, 1))\n",
    "        cond_input = layers.Input(shape = (62,))\n",
    "        cond_up = layers.Dense(128*128, activation='relu')(cond_input)\n",
    "        cond_up = layers.Reshape((128,128,1))(cond_up)\n",
    "\n",
    "        concatenated_input = layers.Concatenate(axis=-1)([image_input, cond_up])\n",
    "\n",
    "        model = models.Sequential()\n",
    "        model.add(Conv2DCircularPadding(16, kernel_size=3, strides=1, input_shape=(128, 128, 2)))\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2DCircularPadding(32, kernel_size=3, strides=2))\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2DCircularPadding(64, kernel_size=7, strides=2))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2DCircularPadding(128, kernel_size=9, strides=2))\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2DCircularPadding(256, kernel_size=3, strides=2))\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        # model.add(Dropout(0.10))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(1, activation='linear'))\n",
    "        return models.Model(inputs = [image_input, cond_input], outputs = model(concatenated_input))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.model(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538930b1-809e-453f-8a98-01d8013e02ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GAN(models.Model):\n",
    "    def __init__(self, generator, discriminator, data, latent_dim, batch_size):\n",
    "        \"\"\"\n",
    "        Définition du modèle complet générateur + discriminateur\n",
    "        :param generator: le générateur\n",
    "        :param discriminator: le disriminateur\n",
    "        :param data_path: Le chemin d'accès aux données prétraitées\n",
    "        \"\"\"\n",
    "        super(GAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.discriminator_optimizer = RMSprop(learning_rate=0.00005)\n",
    "        self.generator_optimizer= RMSprop(learning_rate=0.005)\n",
    "        self.compile_discriminator()\n",
    "        self.compile_gan()\n",
    "        self.data = data\n",
    "        self.latent_dim = latent_dim\n",
    "        self.cond_dim = 62\n",
    "        self.image_dim = (128,128)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "\n",
    "    def gradient_penalty_loss(self, real_samples, fake_samples,X_cond):\n",
    "        \n",
    "        lambda_gp = 10.0\n",
    "    \n",
    "        #1. Création des échantillons interpolés\n",
    "        alpha = tf.random.uniform(shape=[self.batch_size] + [1]*(real_samples.ndim-1), \n",
    "                            minval=0.0, maxval=1.0)\n",
    "        interpolated = alpha * real_samples + (1 - alpha) * fake_samples\n",
    "    \n",
    "        # 2. Calcul du gradient\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(interpolated)  \n",
    "            pred = self.discriminator([interpolated,X_cond], training=True)\n",
    "    \n",
    "        gradients = tape.gradient(pred, [interpolated])[0]\n",
    "    \n",
    "        # 3. Calcul de la pénalité\n",
    "        if gradients is not None:\n",
    "            gradients_sqr = tf.square(gradients)\n",
    "            gradients_sqr_sum = tf.reduce_sum(gradients_sqr, \n",
    "                                       axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "            gradient_l2_norm = tf.sqrt(gradients_sqr_sum)\n",
    "            gradient_penalty = tf.reduce_mean((gradient_l2_norm - 1.0)**2)\n",
    "            print(\"Gradients l2\", gradient_l2_norm)\n",
    "            return lambda_gp * gradient_penalty\n",
    "        else:\n",
    "            print(\"Gradient nul\")\n",
    "            return 0.0\n",
    "        \n",
    "    def compile_discriminator(self):\n",
    "        self.discriminator.compile(loss=self.gradient_penalty_loss, optimizer=RMSprop(learning_rate=0.00005), metrics=['accuracy'])\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "    def compile_gan(self):\n",
    "        latent = layers.Input(shape=(self.generator.latent_dim,))\n",
    "        cond = layers.Input(shape=(self.generator.cond_dim,))\n",
    "        fake_image = self.generator([latent, cond])\n",
    "        input_disc = [fake_image, cond]\n",
    "        validity = self.discriminator(input_disc)\n",
    "\n",
    "        self.model = models.Model([latent,cond], validity)\n",
    "        self.model.compile(loss=generator_loss, optimizer=RMSprop(learning_rate=0.005))\n",
    "\n",
    "    def generate_latent_points(self, latent_dim, n_samples):\n",
    "        \"\"\"\n",
    "        Génère n_samples vecteurs latents\n",
    "        :param latent_dim: dimension de l'espace latent\n",
    "        :param n_samples: taille de l'échantillon crée\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x_input = np.random.randn(latent_dim * n_samples)\n",
    "        x_input = x_input.reshape(n_samples, latent_dim)\n",
    "        return x_input\n",
    "\n",
    "    def generate_cond(self, cond):\n",
    "        \"\"\"\n",
    "        Mise en forme de la condition avant passage dans le modèle\n",
    "        :param cond: Liste des conditions du batch correspondant lors de l'entraînement\n",
    "        :return: les conditions mises en forme\n",
    "        \"\"\"\n",
    "        x_input = pd.concat(cond, axis=1)\n",
    "        x_input = x_input.T\n",
    "        x_input = np.array(x_input)\n",
    "        return x_input\n",
    "\n",
    "    def generate_real_samples(self, n_samples):\n",
    "        \"\"\"\n",
    "        \n",
    "        Charge un batch d'images réelles\n",
    "        :param n_samples: taille du batch\n",
    "        :return: Batch d'images réelles\n",
    "        \"\"\"\n",
    "        dfs = self.data\n",
    "        dfs_array = [(df[0].to_numpy(), df[0].to_numpy()) for df in dfs] ### /!\\\n",
    "        # np.random.shuffle(dfs_array)\n",
    "\n",
    "        sampled_indices = np.random.choice(len(dfs_array), size=n_samples, replace=False)\n",
    "\n",
    "        # Sélectionner les arrays échantillonnés\n",
    "        real_samples = [dfs_array[i] for i in sampled_indices]\n",
    "        real_samples = np.stack(real_samples, axis=0)\n",
    "        real_samples = np.expand_dims(real_samples, axis=-1)\n",
    "        labels = -(np.ones((n_samples, 1))) # Création des labels associés aux images réelles\n",
    "\n",
    "        return real_samples, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8771b0c-9f4d-4b7f-bdbe-f927ec3272f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_training_history(d_losses, g_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(d_losses, label='Discriminator Loss', linestyle='--')\n",
    "    plt.plot(g_losses, label='Generator Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('WGAN Training History')\n",
    "    plt.show()\n",
    "\n",
    "def generate_and_save_outputs(root_folder, gan, latent_dim):\n",
    "    \"\"\"\n",
    "    CHarge des signatures de root_folder puis génère les structures associées selon le gan\n",
    "    :param root_folder:\n",
    "    :param gan:\n",
    "    :param latent_dim:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Parcourir tous les sous-dossiers\n",
    "    for folder in os.listdir(root_folder):\n",
    "        folder_path = os.path.join(root_folder, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            condition_file = os.path.join(folder_path, 'craft_s12.parquet')\n",
    "\n",
    "            # Vérifier si le fichier de condition existe\n",
    "            if os.path.exists(condition_file):\n",
    "                # Charger le DataFrame de condition\n",
    "                condition = pd.read_parquet(condition_file)\n",
    "                condition = condition.T\n",
    "                # Générer un vecteur latent aléatoire\n",
    "                latent_vector = gan.generate_latent_points(latent_dim, 1)\n",
    "\n",
    "                # Générer une sortie du GAN\n",
    "                generated_output = gan.generator.predict([latent_vector, condition])\n",
    "\n",
    "                # Sauvegarder la sortie dans le même dossier\n",
    "                output_file = os.path.join(folder_path, 'generated_output_red.npy')\n",
    "                np.save(output_file, generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90af329-ff62-431a-913a-4833274b460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wgan_gp(generator, discriminator, gan, data, latent_dim, n_epochs, n_critic, batch_size):\n",
    "    \"\"\"\n",
    "    Fonction d'entraînement du GAN\n",
    "    :param generator: le générateur\n",
    "    :param discriminator: le discriminateur\n",
    "    :param gan: le modèle gan complet\n",
    "    :param latent_dim: idem plus haut\n",
    "    :param n_epochs: nombre d'epochs pour l'entraînement\n",
    "    :param n_critic: nombre de mises à jour des poids du discriminateur pour chaque mise à jour des poids du générateur\n",
    "    :param batch_size: taille des batchs lors de l'entraînement\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    d_losses, g_losses = [], [] # stocker les pertes\n",
    "    current_epoch = 0 # compteur d'epochs\n",
    "\n",
    "    #gan.discriminator.compile(loss=gan.gradient_penalty_loss, optimizer=RMSprop(learning_rate=0.00005), metrics=['accuracy'])\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # génération des batchs\n",
    "        batches = generate_batches_cond(data, batch_size)\n",
    "        current_epoch += 1\n",
    "\n",
    "        for batch in batches:\n",
    "            # Update the critic (discriminator) n_critic times\n",
    "            for _ in range(n_critic):\n",
    "\n",
    "                y_real = -(np.ones((batch_size, 1))) # Association des bonnes étiquettes (-1) pour les données réelles\n",
    "\n",
    "                voxel_data = [np.expand_dims(sample[0], axis=-1) for sample in batch]\n",
    "                signature_data = [sample[1].to_numpy() for sample in batch]\n",
    "                \n",
    "                X_real_tab_np = np.array(voxel_data)  # Convertir en numpy array\n",
    "                X_real_sign_np = np.array(signature_data)\n",
    "                \n",
    "                discriminator.trainable = True\n",
    "\n",
    "                # entraînement du discriminateur sur échantillons réels\n",
    "                #d_loss_real = discriminator.train_on_batch([X_real_tab_np, X_real_sign_np], y_real)\n",
    "\n",
    "                # Mesure de la précision sur faux échantillons\n",
    "                latent = gan.generate_latent_points(latent_dim, batch_size)  # génération des vecteurs latents\n",
    "                cond = [sample[1] for sample in batch] # extraction des conditions\n",
    "                cond = gan.generate_cond(cond = cond)  # mise en forme\n",
    "                X_fake = generator.predict([latent, cond]) # Génération des fausses images\n",
    "                plt.imshow(X_fake[0], cmap='gray')\n",
    "                plt.show()\n",
    "                \n",
    "                X_fake_tab_np = np.array([np.array(df) for df in X_fake])\n",
    "                X_fake_sign = np.expand_dims(cond, axis=-1)\n",
    "                \n",
    "                #d_loss_fake = discriminator.train_on_batch([X_fake_tab_np, X_fake_sign], np.ones((batch_size, 1)))  # Utiliser +1  comme cible pour les fausses images\n",
    "                #d_loss = np.mean(np.array(d_loss_fake) - np.array(d_loss_real))\n",
    "                with tf.GradientTape() as d_tape:\n",
    "                    pred_real = discriminator([X_real_tab_np, X_real_sign_np], training=True)\n",
    "                    pred_fake = discriminator([X_fake_tab_np, X_fake_sign], training=True)\n",
    "                \n",
    "                    wasserstein_loss=tf.reduce_mean(pred_fake*(-1))+tf.reduce_mean(pred_real*(1))\n",
    "                    # 2. Ajout du gradient penalty\n",
    "                    gp_loss = gan.gradient_penalty_loss(X_real_tab_np, X_fake_tab_np,X_real_sign_np)\n",
    "                    total_loss = wasserstein_loss + gp_loss\n",
    "                    print(\"pred fake et pred_real\",pred_fake,pred_real)\n",
    "                d_gradients = d_tape.gradient(total_loss, discriminator.trainable_variables)\n",
    "                gan.discriminator_optimizer.apply_gradients(zip(d_gradients, discriminator.trainable_variables))\n",
    "                \n",
    "\n",
    "            discriminator.trainable = False\n",
    "            #gan.model.compile(loss=generator_loss, optimizer=RMSprop(learning_rate=0.005))\n",
    "            # Update the generator\n",
    "            latent = gan.generate_latent_points(latent_dim, batch_size)\n",
    "            cond = [sample[1] for sample in batch]\n",
    "            cond = gan.generate_cond(cond)\n",
    "            y_gan = np.ones((batch_size, 1))\n",
    "            with tf.GradientTape() as g_tape:\n",
    "                X_gen = generator([latent, cond], training=True)\n",
    "                pred_gen = discriminator([X_gen, cond])\n",
    "                g_loss = -tf.reduce_mean(pred_gen*(-1))\n",
    "            \n",
    "            g_gradients = g_tape.gradient(g_loss, generator.trainable_variables)\n",
    "            gan.generator_optimizer.apply_gradients(zip(g_gradients, generator.trainable_variables))\n",
    "\n",
    "            # Record losses for visualization\n",
    "            d_losses.append(total_loss)\n",
    "            g_losses.append(g_loss)\n",
    "\n",
    "        # Sélectionner aléatoirement deux indices de conditions\n",
    "        random_indices = np.random.choice(cond.shape[0], 2, replace=False)\n",
    "        conditions_selected = cond[random_indices]\n",
    "\n",
    "        # Générer des points latents\n",
    "        latent_points = gan.generate_latent_points(latent_dim, 2)  # Pour deux échantillons\n",
    "\n",
    "        # Générer des images avec le générateur\n",
    "        generated_images = gan.generator.predict([latent_points, conditions_selected])\n",
    "\n",
    "        # Afficher les images\n",
    "        for i in range(2):\n",
    "            plt.imshow(generated_images[i], cmap='gray')\n",
    "            plt.show()\n",
    "\n",
    "        # Display results \n",
    "        plot_training_history(d_losses, g_losses)\n",
    "        print(f\"Epoch {current_epoch}, [D loss: {d_losses[-1]}], [G loss: {g_loss}]\")\n",
    "        generator_weights = [layer.get_weights()[0].flatten() for layer in generator.layers if len(layer.get_weights()) > 0]\n",
    "        discriminator_weights = [layer.get_weights()[0].flatten() for layer in discriminator.layers if len(layer.get_weights()) > 0]\n",
    "        \n",
    "        plt.figure(figsize=(10,5))\n",
    "        for weights in generator_weights :\n",
    "            plt.hist(weights, bins = 50, alpha = 0.5)\n",
    "        plt.title('Histogramme des poids du générateur')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10,5))\n",
    "        for weights in discriminator_weights :\n",
    "            plt.hist(weights, bins = 50, alpha = 0.5)\n",
    "        plt.title('Histogramme des poids du discriminateur')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fde623a-f2af-4849-8ec1-23c2a19850d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_parquet_files_cond_red' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[0;32m      7\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/Bader/Desktop/Mines 2A/Projet 2A/CODES/donnees/Database/results\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Définir le chemin d'accès aux données\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload_parquet_files_cond_red\u001b[49m(data_path, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_parquet_files_cond_red' is not defined"
     ]
    }
   ],
   "source": [
    "# Définir le nombre d'époques et la taille du lot\n",
    "latent_dim = 38\n",
    "n_epochs = 200\n",
    "batch_size = 64\n",
    "n_critic = 1\n",
    "import shutil\n",
    "data_path = '/Users/Bader/Desktop/Mines 2A/Projet 2A/CODES/donnees/Database/results'  # Définir le chemin d'accès aux données\n",
    "data = load_parquet_files_cond_red(data_path, test = False)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea12f9f4-402a-47f7-a843-75bb3104e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer les instances des classes\n",
    "\n",
    "generator = Generator(latent_dim, 62)\n",
    "# generator.summary()\n",
    "discriminator = Discriminator(62)\n",
    "# discriminator.summary()\n",
    "gan = GAN(generator, discriminator, data, latent_dim, batch_size)\n",
    "\n",
    "generator.summary()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f517c0-5d67-4aad-b304-10e71de6c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_weights = [layer.get_weights()[0].flatten() for layer in generator.layers if len(layer.get_weights()) > 0]\n",
    "discriminator_weights = [layer.get_weights()[0].flatten() for layer in discriminator.layers if len(layer.get_weights()) > 0]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "for weights in generator_weights :\n",
    "    plt.hist(weights, bins = 50, alpha = 0.5)\n",
    "plt.title('Histogramme des poids du générateur')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "for weights in discriminator_weights :\n",
    "    plt.hist(weights, bins = 50, alpha = 0.5)\n",
    "plt.title('Histogramme des poids du discriminateur')\n",
    "plt.show()\n",
    "\n",
    "# Entraîner le GAN\n",
    "train_wgan_gp(generator, discriminator, gan, data, latent_dim, n_epochs, n_critic, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715c9e3-d5af-4c4b-82c3-25eebe3e3246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le générateur et le discriminateur après l'entraînement\n",
    "generator.model.save('C:/Users/Bader/Desktop/Mines 2A/Projet 2A/Ines/model2/generateur_red_gp2.keras')\n",
    "discriminator.model.save('C:/Users/Bader/Desktop/Mines 2A/Projet 2A/Ines/model2/discriminateur_red_gp2.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
